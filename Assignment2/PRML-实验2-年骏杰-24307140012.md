# **模式识别与机器学习 -- 实验2**

本实验包含以下部分：

-   softmax （50%） 

-   svm （50%）


## **softmax**

1 手动实现 Softmax 函数 （15%）
```python
def my_softmax(logits):
    max_logits, _ = logits.max(dim=-1, keepdim=True)
    exp_logits = torch.exp(logits - max_logits)
    sum_exp_logits = exp_logits.sum(dim=-1, keepdim=True)
    probs = exp_logits / sum_exp_logits
    return probs
```
说明
   使用 logits.max(dim=-1, keepdim=True) 找到每行的最大值
   减去最大值后再计算 exp
   使用exp_logits.sum(dim=-1, keepdim=True)求行和
   除以行和得到softmax归一结果


2 创建自定义 Softmax 层  （15%）
```python
class MySoftmax(nn.Module):
    def __init__(self, dim=-1):
        super().__init__()
        self.dim = dim
    
    def forward(self, x):
        return my_softmax(x)
```
说明
    继承 nn.Module 类
    在 __init__ 方法中初始化维度参数
    在 forward 方法中调用自定义的 my_softmax 函数

3 参数调优实验（无需给出代码）

4 提交实验结果，只需截图最后的实验结果汇总和最佳模型的配置即可（20%）
![alt text](results_summary.png)
![alt text](best_model.png)


## **svm**

### 一、损失和梯度的计算
#### 1, 循环实现中的梯度计算（10%）

补全 fduml.linear_svm import中的svm_loss_naive函数

在这里写代码，并简单说明（注意，没有说明会适当扣分）

```python
def svm_loss_naive(W, X, y, reg):
    """
    结构化SVM损失函数，朴素实现（使用循环）。

    输入数据的维度为D，共有C个类别，我们在包含N个样本的小批量数据上操作。

    输入参数:
    - W: 形状为(D, C)的numpy数组，包含权重。
    - X: 形状为(N, D)的numpy数组，包含一个小批量数据。
    - y: 形状为(N,)的numpy数组，包含训练标签；y[i] = c 表示
      X[i]的标签为c，其中 0 <= c < C。
    - reg: (float) 正则化强度

    返回一个元组:
    - loss: 单个浮点数，表示损失值
    - gradient: 关于权重W的梯度；与W形状相同的数组
    """
    dW = np.zeros(W.shape)

    num_classes = W.shape[1]
    num_train = X.shape[0]
    loss = 0.0
    for i in range(num_train):
        scores = X[i].dot(W)
        correct_class_score = scores[y[i]]
        for j in range(num_classes):
            if j == y[i]:
                continue
            margin = scores[j] - correct_class_score + 1  # note delta = 1
            if margin > 0:
                loss += margin

    loss /= num_train

    loss += reg * np.sum(W * W)

    #############################################################################
    # TODO:                                                                     #
    # 计算损失函数的梯度并将其存储在dW中。                                         #
    # 在计算损失的同时计算导数可能更简单                                           #
    # 可以修改上面的一些代码来计算梯度                                         # 
    #############################################################################
    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****

    for i in range(num_train):
        scores = X[i].dot(W)
        correct_class_score = scores[y[i]]
        for j in range(num_classes):
            if j == y[i]:
                continue
            margin = scores[j] - correct_class_score + 1  # note delta = 1
            if margin > 0:
                dW[:, j] += X[i]
                dW[:, y[i]] -= X[i]
    dW /= num_train
    dW += 2 * reg * W
    

    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****

    return loss, dW
```
说明
    在计算损失的同时，更新梯度dW
    对于每个样本i和每个类别j，如果margin>0，则对dW进行相应的增减
    最后对dW进行平均并加上正则化项的梯度

#### 2, 向量实现中的损失计算和梯度计算（15%）

补全 fduml.linear_svm import中的svm_loss_vectorized函数

在这里写代码，并简单说明
```python
def svm_loss_vectorized(W, X, y, reg):
    """
    结构化SVM损失函数，向量化实现。

    输入和输出与svm_loss_naive相同。
    """
    loss = 0.0
    dW = np.zeros(W.shape)

    #############################################################################
    # TODO:                                                                     #
    # 实现结构化SVM损失的向量化版本，将结果存储在loss中。                           #
    #############################################################################
    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****

    num_train = X.shape[0]
    scores = X.dot(W)  # (N, C)
    correct_class_scores = scores[np.arange(num_train), y].reshape(-1, 1) 
    margins = np.maximum(0, scores - correct_class_scores + 1)
    margins[np.arange(num_train), y] = 0  # 不计算正确类别的margin
    loss = np.sum(margins) / num_train
    loss += reg * np.sum(W * W)


    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****

    #############################################################################
    # TODO:                                                                     #
    # 实现SVM损失梯度的向量化版本，将结果存储在dW中。                               #
    # 提示：与其从头计算梯度，重用一些损失计算时的中间值可能更容易                   #
    #############################################################################
    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****

    binary = margins
    binary[margins > 0] = 1
    row_sum = np.sum(binary, axis=1)
    binary[np.arange(num_train), y] = -row_sum
    dW = X.T.dot(binary) / num_train
    dW += 2 * reg * W

    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****

    return loss, dW
```
说明
    计算所有样本的分数矩阵scores
    提取正确类别的分数correct_class_scores并进行广播
    计算margin矩阵，并将正确类别的margin设为0
    计算总损失并加上正则化项
    使用binary矩阵标记哪些margin>0，并计算每行的和
    更新dW并加上正则化项的梯度

#### 3, 在这里提交ipynb中的相关检查结果（不占额外分数，但这是判断上面的实现是否正确的重要依据
![alt text](test_correctness.png)
![alt text](test_correctness_1.png)

### 二、实现SGD 
代码+简单说明（10%）

```python
class LinearClassifier(object):
    def __init__(self):
        self.W = None

    def train(
        self,
        X,
        y,
        learning_rate=1e-3,
        reg=1e-5,
        num_iters=100,
        batch_size=200,
        verbose=False,
    ):
        """
        使用随机梯度下降训练该线性分类器。

        输入:
        - X: 形状为 (N, D) 的 numpy 数组，包含训练数据；有 N 个训练样本，每个样本的维度为 D。
        - y: 形状为 (N,) 的 numpy 数组，包含训练标签；y[i] = c 表示 X[i] 的标签为 0 <= c < C，其中 C 是类别数。
        - learning_rate: (float) 优化的学习率。
        - reg: (float) 正则化强度。
        - num_iters: (integer) 优化时的迭代步数
        - batch_size: (integer) 每一步使用的训练样本数量。
        - verbose: (boolean) 如果为 true，在优化过程中打印进度。

        输出:
        包含每次训练迭代中损失函数值的列表。
        """
        num_train, dim = X.shape
        num_classes = (
            np.max(y) + 1
        )  # 假设 y 的取值为 0...K-1，其中 K 是类别数
        if self.W is None:
            # 延迟初始化 W
            self.W = 0.001 * np.random.randn(dim, num_classes)

        # 运行随机梯度下降以优化 W
        loss_history = []
        for it in range(num_iters):
            X_batch = None
            y_batch = None

            #########################################################################
            # TODO:                                                                 #
            # 从训练数据中采样 batch_size 个元素及其对应的标签，用于本轮梯度下降。        #
            # 将数据存储在 X_batch 中，对应的标签存储在 y_batch 中；采样后 X_batch 的     #
            # 形状应为 (batch_size, dim)，y_batch 的形状应为 (batch_size,)            #
            #                                                                       #
            # 提示: 使用 np.random.choice 生成索引。有放回采样比无放回采样更快。          #
            #########################################################################
            # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****

            indices = np.random.choice(num_train, batch_size, replace=True)
            X_batch = X[indices]
            y_batch = y[indices]

            # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****

            # 评估损失和梯度
            loss, grad = self.loss(X_batch, y_batch, reg)
            loss_history.append(loss)

            # 执行参数更新
            #########################################################################
            # TODO:                                                                 #
            # 使用梯度和学习率更新权重。                                              #
            #########################################################################
            # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****

            self.W -= learning_rate * grad

            # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****

            if verbose and it % 100 == 0:
                print("iteration %d / %d: loss %f" % (it, num_iters, loss))

        return loss_history

    def predict(self, X):
        """
        使用该线性分类器的训练权重预测数据点的标签。

        输入:
        - X: 形状为 (N, D) 的 numpy 数组，包含训练数据；有 N 个训练样本，每个样本的维度为 D。

        返回:
        - y_pred: X 中数据的预测标签。y_pred 是一个长度为 N 的一维数组，每个元素是一个整数，表示预测的类别。
        """
        y_pred = np.zeros(X.shape[0])
        ###########################################################################
        # TODO:                                                                   #
        # 实现此方法。将预测标签存储在 y_pred 中。                                    #
        ###########################################################################
        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****

        scores = X.dot(self.W) 
        y_pred = np.argmax(scores, axis=1)

        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****
        return y_pred

    def loss(self, X_batch, y_batch, reg):
        """
        计算损失函数及其导数。
        子类将重写此方法。

        输入:
        - X_batch: 形状为 (N, D) 的 numpy 数组，包含一个包含 N 个数据点的小批量；每个点的维度为 D。
        - y_batch: 形状为 (N,) 的 numpy 数组，包含小批量的标签。
        - reg: (float) 正则化强度。

        返回: 包含以下内容的元组:
        - loss 作为单个浮点数
        - 关于 self.W 的梯度；与 W 形状相同的数组
        """
        pass
```
说明
    定义 LinearClassifier 类，包含初始化、训练、预测和损失计算方法
    在 train 方法中实现随机梯度下降，包括随机批量采样、损失和梯度计算以及权重更新
    在 predict 方法中使用当前权重进行预测，预测类别为得分最高的类别
    loss 方法作为占位符，子类将实现具体的损失函数

### 三、利用验证集做超参数调优 
表格记录（5*5），可以自己尝试不同的组合 （10%）

| 学习率 \ 正则化强度 | 1e4       | 2.5e4        | 5e4         | 1e5         | 2.5e5        |
| :----------------- | :---------- | :---------- | :---------- | :---------- | :---------- |
| **1e-8**         | 0.196      | 0.176      | 0.190      | 0.200      | 0.269      |
| **5e-8**          | 0.237      | 0.281      | 0.327      | 0.337      | 0.345      |
| **1e-7**           | 0.307      | 0.339      | 0.343     | 0.324      | 0.322      |
| **5e-7**            | 0.335      | 0.326      | 0.317      | 0.315      | 0.273      |
| **1e-6**            | 0.342      | 0.295      | 0.241      | 0.228      | 0.209     |

最优的结果的loss曲线截图和正确率截图（5%）
![alt text](best_params_loss_curve.png)


